{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database with Location\n",
    "- As we saw, scraping by tag allows us to obtain recent posts\n",
    "- But we need to scrape location information and also download the images for each media post.\n",
    "- That's a lot of pages to scrape!\n",
    "\n",
    "\n",
    "- As we established, scraping too fast will get us blocked. The recommended speed limit is also very slow.\n",
    "- Allowing 15-30s rest per request, we can scrape ~5000 images a day if we scrape all day. It's good enough for us for our MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't worry about these\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "sys.path.append('..')\n",
    "import pathlib\n",
    "from time import sleep\n",
    "\n",
    "from igramscraper.instagram import Instagram\n",
    "from igramscraper.exception.instagram_not_found_exception import InstagramNotFoundException\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "%matplotlib inline  \n",
    "\n",
    "# import previously defined functions\n",
    "from core.utils import get_thumbnail, show_thumbnail, imresize\n",
    "from core.instagram import get_media_by_url\n",
    "from core.envs import DATA_DIR, IMAGE_DIR, THUMBNAIL_DIR\n",
    "from core.db.persistence import media_to_row, DATA_ATTRIBUTES\n",
    "\n",
    "# change paths\n",
    "PROJECT_TT_ROOT = pathlib.Path('../..')\n",
    "DATA_DIR = PROJECT_TT_ROOT / DATA_DIR\n",
    "IMAGE_DIR = PROJECT_TT_ROOT / IMAGE_DIR\n",
    "THUMBNAIL_DIR = PROJECT_TT_ROOT / THUMBNAIL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify target directory where we will save data\n",
    "search_tag = 'london'  # the tag we use for scraping\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M\")  # log the current time\n",
    "dest_dir = DATA_DIR / search_tag / timestamp  # create dataset based on search tag and timestamp\n",
    "dest_img_dir = dest_dir / 'images'\n",
    "dest_thumbnail_dir = dest_dir / 'thumbnails'\n",
    "\n",
    "# create directory if they don't exist\n",
    "if not dest_dir.exists():\n",
    "    os.makedirs(dest_dir)\n",
    "    os.makedirs(dest_img_dir)\n",
    "    os.makedirs(dest_thumbnail_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get bunch of posts using our usual function. For this example, lets scrape 10 medias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instagram = Instagram(sleep_between_requests=15)\n",
    "medias = instagram.get_medias_by_tag(search_tag, count=10)  # this will take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas.Dataframe from scraped data\n",
    "df = pd.DataFrame([media_to_row(m) for m in medias], columns=DATA_ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from the dataframe\n",
    "csv_name = dest_dir / f'{search_tag}_{timestamp}.csv'\n",
    "df.to_csv(csv_name, quotechar=\"'\", index=False)\n",
    "print(f'saved to {csv_name}')                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created a temporary dataset, lets scrape the location information for each media with all the attributes that we want, using the `get_media_by_url` function we defined in \"[2] Instagram Data Attributes.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medias = pd.read_csv(csv_name, quotechar=\"'\")  # read csv is easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_list = []\n",
    "full_medias = []\n",
    "progress_idx = 0\n",
    "\n",
    "# create csv to save the new dataset with all attributes\n",
    "fname = dest_dir / f'{search_tag}_{timestamp}_location.csv'\n",
    "\n",
    "# create csv file with just the column names. In the loop, we will add one row at a time\n",
    "df_full_medias = pd.DataFrame([], columns=DATA_ATTRIBUTES)\n",
    "df_full_medias.to_csv(fname, quotechar=\"'\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for curr_idx in range(progress_idx+1, len(medias)):\n",
    "    media = medias.iloc[curr_idx]\n",
    "    print(f'[{curr_idx}/{len(medias)}]: {media.media_id}')\n",
    "\n",
    "    # sometimes the request can fail so surround in try-catch block\n",
    "    try:\n",
    "        # scrape media\n",
    "        sleep(np.random.randint(15))\n",
    "        print(f'.. scrape media {media.media_link}')\n",
    "        media_obj = get_media_by_url(instagram, media.media_link)\n",
    "\n",
    "        # scrape media image\n",
    "        sleep(np.random.randint(15))\n",
    "        print(f'.. scrape image {media.img_thumbnail_url}')\n",
    "        thumbnail = get_thumbnail(media.img_thumbnail_url)\n",
    "\n",
    "        # save the images\n",
    "        imname = f'{dest_img_dir}/{media.media_id}.jpeg'\n",
    "        thumbnail_name = f'{dest_thumbnail_dir}/{media.media_id}.jpeg'\n",
    "        Image.fromarray(thumbnail).save(imname)\n",
    "        Image.fromarray(imresize(thumbnail, (64,64))).save(thumbnail_name)\n",
    "\n",
    "        # \n",
    "        datarow = media_to_row(media_obj)\n",
    "        full_medias.append(datarow)\n",
    "        df_full_medias = df_full_medias.append(datarow)\n",
    "\n",
    "        # append the row to csv\n",
    "        df_row = pd.DataFrame([datarow])\n",
    "        df_row.to_csv(fname, quotechar=\"'\", mode='a', header=False, index=False)\n",
    "\n",
    "    except InstagramNotFoundException as e:\n",
    "        print(e)\n",
    "        print('.. adding to bad list')\n",
    "        bad_list.append(media)\n",
    "\n",
    "    # keep track of the current index we've got, in case the function fails\n",
    "    progress_idx = curr_idx\n",
    "    \n",
    "    # random long sleep\n",
    "    sleep_time = np.random.randint(60)\n",
    "    print(f'.. sleeping for {sleep_time}s')\n",
    "    sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
